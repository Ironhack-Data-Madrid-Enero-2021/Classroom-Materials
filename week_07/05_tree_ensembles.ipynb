{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Bagging</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-problem\" data-toc-modified-id=\"The-problem-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>The problem</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-metric\" data-toc-modified-id=\"The-metric-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>The metric</a></span></li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Train test split</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Decision-tree-classifier\" data-toc-modified-id=\"Decision-tree-classifier-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Decision tree classifier</a></span></li><li><span><a href=\"#Decision-tree-classifier\" data-toc-modified-id=\"Decision-tree-classifier-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Decision tree classifier</a></span></li></ul></li></ul></li><li><span><a href=\"#Boosting\" data-toc-modified-id=\"Boosting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Boosting</a></span></li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Gradient Boosting</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Decision-tree-classifier\" data-toc-modified-id=\"Decision-tree-classifier-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Decision tree classifier</a></span></li></ul></li></ul></li><li><span><a href=\"#NOTE\" data-toc-modified-id=\"NOTE-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>NOTE</a></span></li><li><span><a href=\"#Further-materials\" data-toc-modified-id=\"Further-materials-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Further materials</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble models in ML are those that:\n",
    " * use several *weak learners* to\n",
    " * build an averaging model than behaves better than the individual ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging (Bootstrap Aggregation) is a Machine Learning tool used to:\n",
    " * improve the stability of an algorithm (robust to little changes in data)\n",
    " * reduce overfitting\n",
    " \n",
    "It consists on, given an original dataset $D$:\n",
    " 1. building different datasets $D_i$ from $D$, extracting samples with replacement (bootstraping)\n",
    " 2. and building a model for each dataset $D_i$\n",
    " 3. for finally averaging predictions in test phase (aggregating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=600 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/440px-Ensemble_Bagging.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest applies the Bagging logic to build several decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=600 src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/rfc_vs_dt1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a simple Logistic Regression, a simple Decision Tree, and then look at the improvements with a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/breast_cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a metric that represents our notion of good fit\n",
    "2. Train-test split\n",
    "3. Train a logistic regression\n",
    "4. Train a decision tree\n",
    "5. Train a random forest\n",
    "6. Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be f3 score for threshold 0.2: recall is 3x important than precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"is_cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(target, axis=1)\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an intuition of what a good metric score is, lets evaluate a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(\n",
    "    y_pred=np.ones(X_test.shape[0]),\n",
    "    y_true=y_test,\n",
    "    beta=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_score = fbeta_score(\n",
    "    y_pred=log.predict_proba(X_test)[:, 1] > threshold,\n",
    "    y_true=y_test,\n",
    "    beta=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The test score with logistic regression is {log_test_score.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_test_score = fbeta_score(\n",
    "    y_pred=tree.predict_proba(X_test)[:, 1] > threshold,\n",
    "    y_true=y_test,\n",
    "    beta=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The test score with decision tree is {tree_test_score.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "plot_tree(tree, feature_names=df.columns[:-1], filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"classif_cancer.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * n_estimators: number of trees\n",
    " * max_depth: max depth of tree\n",
    " * max_features: number of variables to use in each tree (will change across trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=4, max_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_score = fbeta_score(\n",
    "    y_pred=rf.predict_proba(X_test)[:, 1] > threshold,\n",
    "    y_true=y_test,\n",
    "    beta=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The test score with random forest is {rf_test_score.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is another ensemble technique to create a collection of predictors. In this case:\n",
    " * learners (models) are built sequentially\n",
    " * early learners fit simple models to the data\n",
    " * errors are analyzed and taken into account for the subsequent model construction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=600 src=\"https://iq.opengenus.org/content/images/2020/01/boosted-trees-process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting applies Boosting logic (apart from Bagging) to build several decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It fits **consecutive** trees and, at every step\n",
    " * weights the data samples differently\n",
    " * to focus on the most difficult entries to be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * n_estimators: number of trees\n",
    " * max_depth: max depth of tree\n",
    " * max_features: number of variables to use in each tree (will change across trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100, max_depth=4, max_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_test_score = fbeta_score(\n",
    "    y_pred=gb.predict_proba(X_test)[:, 1] > threshold,\n",
    "    y_true=y_test,\n",
    "    beta=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The test score with gradient boosting is {gb_test_score.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a pity, our Random Forest or Gradient boosting didn't improve Decision Tree results.  \n",
    "This is not the general case: normally they perform better!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still can do GridSearchCV to find best hyperparams, and maybe improve the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Random Forests **can** be trained in a parallel manner: if you build 1000 trees, you can do it in different machines\n",
    " * Gradient Boosting **cannot** be trained in a parallel manner: trees are built sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this post](https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9) for an overview of the theory of Tree ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[See StatQuest Gradient Boosting](https://www.youtube.com/watch?v=3CC4N4z3GJc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XGBoost](https://medium.com/@jboscomendoza/tutorial-xgboost-en-python-53e48fc58f73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introduction to Statistical Learning ISL (Chapter 2)](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)\n",
    "* [repo from tdpetrou with materials from ISL](https://github.com/tdpetrou/Machine-Learning-Books-With-Python/tree/master/Introduction%20to%20Statistical%20Learning)\n",
    "\n",
    "* CheckOut: XGBoost, LightGBM, CatBoost\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "145.994px",
    "width": "198.991px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216.094px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
