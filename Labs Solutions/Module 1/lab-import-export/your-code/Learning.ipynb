{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Exporting Data\n",
    "Lesson Goals\n",
    "\n",
    "    Learn how to import and export delimited files with Pandas.\n",
    "    Learn how to import and export JSON files.\n",
    "    Learn how to read data from a database.\n",
    "    Learn how to write data to a database.\n",
    "\n",
    "Introduction\n",
    "\n",
    "No analytics tool operates in a vacuum. Most of the time, the systems that generate the data are not the ones where analysis of that data is conducted. Because of this, we must have a way to obtain data from external data sources, load it into Pandas, and also be able to export data or our results for further use or presentation. In this lesson, we will cover ways to import data from, and export data to, a variety of formats and destinations using Pandas.\n",
    "Preparation\n",
    "\n",
    "Download [vehicles.zip](https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/data/module-1/vehicles.zip) which contains a bunch of data files in different file formats. Extract the content to your machine. You should see the following extracted files:\n",
    "\n",
    "vehicles_messy.csv\n",
    "\n",
    "vehicles_pipe.txt\n",
    "\n",
    "vehicles_tab.txt\n",
    "\n",
    "vehicles.csv\n",
    "\n",
    "vehicles.json\n",
    "\n",
    "vehicles.xlsx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing and Exporting Delimited Files\n",
    "\n",
    "One of the most common places where data originates are delimited files. Most analytics applications have the ability to read and process delimited files, so they are a popular way to pass information from one system to another. There a few common file formats you are likely to see out in the real world.\n",
    "\n",
    "    Comma-separated variable (CSV) files\n",
    "    Tab-delimited files\n",
    "    Pipe-delimited files\n",
    "\n",
    "Pandas provides us with the ability to import any of these using the read_csv method. For files delimited with characters other than commas, we just need to specify the type of delimiter via the method's sep parameter so that Pandas knows how it should separate the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# Import comma-separated variable file\n",
    "data1 = pd.read_csv('vehicles/vehicles.csv')\n",
    "\n",
    "# Import tab-delimited file\n",
    "data2 = pd.read_csv('vehicles/vehicles_tab.txt', sep='\\t')\n",
    "\n",
    "# Import pipe-delimited file\n",
    "data3 = pd.read_csv('vehicles/vehicles_pipe.txt', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting data as delimited files is just as easy. Instead of using the read_csv method, you use to_csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comma-separated variable file\n",
    "data4 = data1.to_csv('vehicles/vehicles.csv', index=False)\n",
    "\n",
    "# Export tab-delimited file\n",
    "data5 = data2.to_csv('vehicles/vehicles_tab.txt', sep='\\t', index=False)\n",
    "\n",
    "# Export pipe-delimited file\n",
    "data6 = data3.to_csv('vehicles/vehicles_pipe.txt', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we set the index parameter to False. If we did not do that, it would export the data frame with an extra column containing its indexes. Since the indexes have no meaning to us in this case, we are going to exclude them from our export.\n",
    "\n",
    "\n",
    "\n",
    "# Importing and Exporting Excel\n",
    "\n",
    "We can also import and export Microsoft Excel spreadsheets with Pandas. The way to do this is similar to how we imported and exported delimited files, but instead of read_csv and to_csv, we will use the read_excel and to_excel methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('vehicles/vehicles.xlsx')\n",
    "\n",
    "data.to_excel('vehicles/vehicles.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Exporting JSON\n",
    "\n",
    "Another common format for importing and exporting data is JSON. JSON stands for Javascript Object Notation, and it allows you to format data in intuitive ways so that it can be easily read and processed. We can use Pandas to read and write JSON files as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('vehicles/vehicles.json', orient='records')\n",
    "\n",
    "data.to_json('vehicles/vehicles.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we set the orient parameter to 'records' in our code examples above. We did this because our JSON file was structured as a list of dictionaries where each dictionary represented a complete record of data. When working with JSON files in Pandas, the way the data is organized is going to dictate the value you pass to the orient parameter. Below are a few other common ways that JSON files can be structured and the corresponding value you should pass to the orient parameter for each one.\n",
    "\n",
    "    'split': Dictionary containing indexes, columns, and data.\n",
    "    'index': Nested dictionaries containing {index:{column:value}}.\n",
    "    'columns': Nested dictionaries containing {column:{index:value}}\n",
    "    'values': Nested list where each sublist contains the values for a record.\n",
    "    'table': Nested dictionaries containing schema and data (records).\n",
    "\n",
    "Challenge: Try exporting the data passing each of these values to the orient parameter. Open each of the files in a text editor and note the differences in structure.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reading Data from Databases\n",
    "\n",
    "In addition to reading data from various types of files, Pandas also provides us with the ability to read data from MySQL databases. To do so, we need to import the pymysql library and the create_engine function from the sqlalchemy library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must then call the create_engine function and pass it the string below, replacing username and password with the actual username and password for the MySQL database on your local machine. We will assign the result to a variable called engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine=create_engine('mysql+mysqlconnector://admin:password@localhost:3306/Apps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we can use the Pandas read_sql_query function, pass it a SQL statement, and specify that it is to run that statement on the engine connection we created to our MySQL database. In the example below, we are querying all records from the Ratings table in our Apps database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql_query('SELECT * FROM Ratings', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data to Databases\n",
    "\n",
    "Once you have data in a data frame and you have your MySQL database connections saved to the engine variable, writing the data to a table in the database is pretty straightforward. You can use Pandas' to_sql method and specify the table name you want to give the data set, the database connection, what you want to happen if the table already exists (replace, append, fail, etc.) and whether you want to include or exclude the indexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_sql('Ratings2', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you refresh the publications database, you should now see a table named \"Ratings2.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
