{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delegation Coordinator (prior health plan expe...</td>\n",
       "      <td>Blue Cross Blue Shield of Arizona</td>\n",
       "      <td>Phoenix, AZ, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Capital One</td>\n",
       "      <td>McLean, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Information Specialist II - Data Analysis</td>\n",
       "      <td>ECRI Institute</td>\n",
       "      <td>Plymouth Meeting, PA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Alliance of Community Health Plans (ACHP)</td>\n",
       "      <td>Washington D.C. Metro Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Principal Data Analyst – Catering Operations A...</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Jobs @ TheJobNetwork</td>\n",
       "      <td>Richmond, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Staff Business Data Analysis</td>\n",
       "      <td>Intuit</td>\n",
       "      <td>San Diego, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JSEG Senior Systems &amp; Data Analysis Engineer</td>\n",
       "      <td>Jacobs</td>\n",
       "      <td>Huntsville, AL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analysis &amp; Evaluation</td>\n",
       "      <td>AmeriCorps</td>\n",
       "      <td>Phillipsburg, NJ, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sales Data and Reporting Analysis</td>\n",
       "      <td>Kemper</td>\n",
       "      <td>Dallas, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Analysis Intern</td>\n",
       "      <td>The Athena Data Company, LLC</td>\n",
       "      <td>Ashland, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Manager, Data Analysis</td>\n",
       "      <td>Beyondsoft</td>\n",
       "      <td>Des Moines, IA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Senior Analyst, Data and Analysis</td>\n",
       "      <td>Digitas North America</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Director Data Analysis</td>\n",
       "      <td>Neustar, Inc.</td>\n",
       "      <td>Sterling, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Scientist - Advanced Media Data Analysis</td>\n",
       "      <td>Frequence</td>\n",
       "      <td>Mountain View, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Research and Data Analysis Intern</td>\n",
       "      <td>SAIC</td>\n",
       "      <td>Reston, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Director Data Analysis - IT Control</td>\n",
       "      <td>Progressive Insurance</td>\n",
       "      <td>Village of Mayfield, OH, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Analysis Consultant</td>\n",
       "      <td>MassMutual</td>\n",
       "      <td>Springfield, MA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Reporting Contractor (Data Analysis)</td>\n",
       "      <td>VIP</td>\n",
       "      <td>Plano, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MANAGER OF DATA ANALYSIS UNIT</td>\n",
       "      <td>Colorado State University</td>\n",
       "      <td>Lakewood, CO, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Analysis Engineer</td>\n",
       "      <td>Citrine Informatics</td>\n",
       "      <td>Redwood City, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Analyst, Data &amp; Analysis</td>\n",
       "      <td>MetaDesign</td>\n",
       "      <td>Chicago, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Principal Engineer - Data and Statistical Anal...</td>\n",
       "      <td>Bausch + Lomb</td>\n",
       "      <td>Rochester, NY, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Research Data Collection and Analysis</td>\n",
       "      <td>Penn State University</td>\n",
       "      <td>University Park, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Analysis</td>\n",
       "      <td>Efficient Frontier</td>\n",
       "      <td>Glendale, CA, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   Delegation Coordinator (prior health plan expe...   \n",
       "1                               Data Analysis Manager   \n",
       "2           Information Specialist II - Data Analysis   \n",
       "3                               Data Analysis Manager   \n",
       "4   Principal Data Analyst – Catering Operations A...   \n",
       "5                               Data Analysis Manager   \n",
       "6                        Staff Business Data Analysis   \n",
       "7        JSEG Senior Systems & Data Analysis Engineer   \n",
       "8                          Data Analysis & Evaluation   \n",
       "9                   Sales Data and Reporting Analysis   \n",
       "10                               Data Analysis Intern   \n",
       "11                             Manager, Data Analysis   \n",
       "12                  Senior Analyst, Data and Analysis   \n",
       "13                             Director Data Analysis   \n",
       "14      Data Scientist - Advanced Media Data Analysis   \n",
       "15                  Research and Data Analysis Intern   \n",
       "16                Director Data Analysis - IT Control   \n",
       "17                           Data Analysis Consultant   \n",
       "18               Reporting Contractor (Data Analysis)   \n",
       "19                      MANAGER OF DATA ANALYSIS UNIT   \n",
       "20                             Data Analysis Engineer   \n",
       "21                           Analyst, Data & Analysis   \n",
       "22  Principal Engineer - Data and Statistical Anal...   \n",
       "23              Research Data Collection and Analysis   \n",
       "24                                      Data Analysis   \n",
       "\n",
       "                                      Company                     Location  \n",
       "0           Blue Cross Blue Shield of Arizona              Phoenix, AZ, US  \n",
       "1                                 Capital One               McLean, VA, US  \n",
       "2                              ECRI Institute     Plymouth Meeting, PA, US  \n",
       "3   Alliance of Community Health Plans (ACHP)   Washington D.C. Metro Area  \n",
       "4                             Delta Air Lines              Atlanta, GA, US  \n",
       "5                        Jobs @ TheJobNetwork             Richmond, VA, US  \n",
       "6                                      Intuit            San Diego, CA, US  \n",
       "7                                      Jacobs           Huntsville, AL, US  \n",
       "8                                  AmeriCorps         Phillipsburg, NJ, US  \n",
       "9                                      Kemper               Dallas, TX, US  \n",
       "10               The Athena Data Company, LLC              Ashland, VA, US  \n",
       "11                                 Beyondsoft           Des Moines, IA, US  \n",
       "12                      Digitas North America             Atlanta, Georgia  \n",
       "13                              Neustar, Inc.             Sterling, VA, US  \n",
       "14                                  Frequence        Mountain View, CA, US  \n",
       "15                                       SAIC               Reston, VA, US  \n",
       "16                      Progressive Insurance  Village of Mayfield, OH, US  \n",
       "17                                 MassMutual          Springfield, MA, US  \n",
       "18                                        VIP                Plano, TX, US  \n",
       "19                  Colorado State University             Lakewood, CO, US  \n",
       "20                        Citrine Informatics         Redwood City, CA, US  \n",
       "21                                 MetaDesign              Chicago, IL, US  \n",
       "22                              Bausch + Lomb            Rochester, NY, US  \n",
       "23                      Penn State University      University Park, TX, US  \n",
       "24                         Efficient Frontier             Glendale, CA, US  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def scrape_linkedin_job_search_pages(keywords, num_pages):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    \n",
    "    # Assemble the full url with parameters, &start=50\n",
    "    \n",
    "    for i in range(num_pages):\n",
    "    \n",
    "        if i==0:\n",
    "            scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "        else:\n",
    "            scrape_url = ''.join([BASE_URL, 'keywords=', keywords, '&start=', '{}'.format((i)*25)])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "\n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Capital One</td>\n",
       "      <td>McLean, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Information Specialist II - Data Analysis</td>\n",
       "      <td>ECRI Institute</td>\n",
       "      <td>Plymouth Meeting, PA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Analyst, Data and Analysis</td>\n",
       "      <td>Digitas North America</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Alliance of Community Health Plans (ACHP)</td>\n",
       "      <td>Washington D.C. Metro Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Population Health Analyst</td>\n",
       "      <td>Tech Finders</td>\n",
       "      <td>Phoenix, Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Principal Data Analyst – Catering Operations A...</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Jobs @ TheJobNetwork</td>\n",
       "      <td>Richmond, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Staff Business Data Analysis</td>\n",
       "      <td>Intuit</td>\n",
       "      <td>San Diego, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JSEG Senior Systems &amp; Data Analysis Engineer</td>\n",
       "      <td>Jacobs</td>\n",
       "      <td>Huntsville, AL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analysis &amp; Evaluation</td>\n",
       "      <td>AmeriCorps</td>\n",
       "      <td>Phillipsburg, NJ, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sales Data and Reporting Analysis</td>\n",
       "      <td>Kemper</td>\n",
       "      <td>Dallas, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analysis Intern</td>\n",
       "      <td>The Athena Data Company, LLC</td>\n",
       "      <td>Ashland, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Manager, Data Analysis</td>\n",
       "      <td>Beyondsoft</td>\n",
       "      <td>Des Moines, IA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Director Data Analysis</td>\n",
       "      <td>Neustar, Inc.</td>\n",
       "      <td>Sterling, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Scientist - Advanced Media Data Analysis</td>\n",
       "      <td>Frequence</td>\n",
       "      <td>Mountain View, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Research and Data Analysis Intern</td>\n",
       "      <td>SAIC</td>\n",
       "      <td>Reston, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Director Data Analysis - IT Control</td>\n",
       "      <td>Progressive Insurance</td>\n",
       "      <td>Village of Mayfield, OH, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Analysis Consultant</td>\n",
       "      <td>MassMutual</td>\n",
       "      <td>Springfield, MA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Reporting Contractor (Data Analysis)</td>\n",
       "      <td>VIP</td>\n",
       "      <td>Plano, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MANAGER OF DATA ANALYSIS UNIT</td>\n",
       "      <td>Colorado State University</td>\n",
       "      <td>Lakewood, CO, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Analysis Engineer</td>\n",
       "      <td>Citrine Informatics</td>\n",
       "      <td>Redwood City, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Analyst, Data &amp; Analysis</td>\n",
       "      <td>MetaDesign</td>\n",
       "      <td>Chicago, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Principal Engineer - Data and Statistical Anal...</td>\n",
       "      <td>Bausch + Lomb</td>\n",
       "      <td>Rochester, NY, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Research Data Collection and Analysis</td>\n",
       "      <td>Penn State University</td>\n",
       "      <td>University Park, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Analysis</td>\n",
       "      <td>Efficient Frontier</td>\n",
       "      <td>Glendale, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Data Analyst – Catering Operations Analysis an...</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>Los Angeles, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Modeling / Predicitve Analysis</td>\n",
       "      <td>Mitchell Martin Inc.</td>\n",
       "      <td>Lewisville, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Analyst, Data and Analysis</td>\n",
       "      <td>Digitas North America</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Analysis Intern</td>\n",
       "      <td>The Athena Data Company, LLC</td>\n",
       "      <td>Ashland, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Alliance of Community Health Plans (ACHP)</td>\n",
       "      <td>Washington D.C. Metro Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Director Data Analysis</td>\n",
       "      <td>Neustar, Inc.</td>\n",
       "      <td>Sterling, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>NYU Langone Health</td>\n",
       "      <td>New York City, NY, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Manager, Data Analysis</td>\n",
       "      <td>Beyondsoft</td>\n",
       "      <td>Des Moines, IA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Data Analysis Engineer</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Seattle, WA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Manager of Data Analysis</td>\n",
       "      <td>Alarm.com</td>\n",
       "      <td>Washington, D.C., DC, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Data Analysis &amp; Evaluation</td>\n",
       "      <td>AmeriCorps</td>\n",
       "      <td>Phillipsburg, NJ, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Director Data Analysis - IT Control</td>\n",
       "      <td>Progressive Insurance</td>\n",
       "      <td>Village of Mayfield, OH, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Data Analysis, Lead</td>\n",
       "      <td>Fiserv</td>\n",
       "      <td>Alpharetta, GA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Research and Data Analysis Intern</td>\n",
       "      <td>SAIC</td>\n",
       "      <td>Reston, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Data Analysis &amp; Fulfillment Specialist - Carlsbad</td>\n",
       "      <td>ClosingCorp</td>\n",
       "      <td>Carlsbad, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>The MENTOR Network</td>\n",
       "      <td>Lawrence, MA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Data Analysis Consultant</td>\n",
       "      <td>MassMutual</td>\n",
       "      <td>Springfield, MA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Genworth</td>\n",
       "      <td>Richmond, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Analyst, Data &amp; Analysis</td>\n",
       "      <td>MetaDesign</td>\n",
       "      <td>Chicago, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Information Specialist II - Data Analysis</td>\n",
       "      <td>ECRI Institute</td>\n",
       "      <td>Plymouth Meeting, PA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Data Analyst – Data &amp; Analysis</td>\n",
       "      <td>Sourcebooks</td>\n",
       "      <td>Naperville, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>JSEG Senior Systems &amp; Data Analysis Engineer</td>\n",
       "      <td>Jacobs</td>\n",
       "      <td>Huntsville, AL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Reporting Contractor (Data Analysis)</td>\n",
       "      <td>VIP</td>\n",
       "      <td>Plano, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sales Data and Reporting Analysis</td>\n",
       "      <td>Kemper</td>\n",
       "      <td>Dallas, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Data Analysis Specialist</td>\n",
       "      <td>Sony Pictures Entertainment</td>\n",
       "      <td>Culver City, California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                               Data Analysis Manager   \n",
       "1           Information Specialist II - Data Analysis   \n",
       "2                   Senior Analyst, Data and Analysis   \n",
       "3                               Data Analysis Manager   \n",
       "4                           Population Health Analyst   \n",
       "5   Principal Data Analyst – Catering Operations A...   \n",
       "6                               Data Analysis Manager   \n",
       "7                        Staff Business Data Analysis   \n",
       "8        JSEG Senior Systems & Data Analysis Engineer   \n",
       "9                          Data Analysis & Evaluation   \n",
       "10                  Sales Data and Reporting Analysis   \n",
       "11                               Data Analysis Intern   \n",
       "12                             Manager, Data Analysis   \n",
       "13                             Director Data Analysis   \n",
       "14      Data Scientist - Advanced Media Data Analysis   \n",
       "15                  Research and Data Analysis Intern   \n",
       "16                Director Data Analysis - IT Control   \n",
       "17                           Data Analysis Consultant   \n",
       "18               Reporting Contractor (Data Analysis)   \n",
       "19                      MANAGER OF DATA ANALYSIS UNIT   \n",
       "20                             Data Analysis Engineer   \n",
       "21                           Analyst, Data & Analysis   \n",
       "22  Principal Engineer - Data and Statistical Anal...   \n",
       "23              Research Data Collection and Analysis   \n",
       "24                                      Data Analysis   \n",
       "25  Data Analyst – Catering Operations Analysis an...   \n",
       "26                Data Modeling / Predicitve Analysis   \n",
       "27                         Analyst, Data and Analysis   \n",
       "28                               Data Analysis Intern   \n",
       "29                              Data Analysis Manager   \n",
       "30                             Director Data Analysis   \n",
       "31                                       Data Analyst   \n",
       "32                             Manager, Data Analysis   \n",
       "33                             Data Analysis Engineer   \n",
       "34                           Manager of Data Analysis   \n",
       "35                         Data Analysis & Evaluation   \n",
       "36                Director Data Analysis - IT Control   \n",
       "37                                Data Analysis, Lead   \n",
       "38                  Research and Data Analysis Intern   \n",
       "39  Data Analysis & Fulfillment Specialist - Carlsbad   \n",
       "40                                       Data Analyst   \n",
       "41                           Data Analysis Consultant   \n",
       "42                              Data Analysis Manager   \n",
       "43                           Analyst, Data & Analysis   \n",
       "44          Information Specialist II - Data Analysis   \n",
       "45                     Data Analyst – Data & Analysis   \n",
       "46       JSEG Senior Systems & Data Analysis Engineer   \n",
       "47               Reporting Contractor (Data Analysis)   \n",
       "48                  Sales Data and Reporting Analysis   \n",
       "49                           Data Analysis Specialist   \n",
       "\n",
       "                                      Company                     Location  \n",
       "0                                 Capital One               McLean, VA, US  \n",
       "1                              ECRI Institute     Plymouth Meeting, PA, US  \n",
       "2                       Digitas North America             Atlanta, Georgia  \n",
       "3   Alliance of Community Health Plans (ACHP)   Washington D.C. Metro Area  \n",
       "4                                Tech Finders             Phoenix, Arizona  \n",
       "5                             Delta Air Lines              Atlanta, GA, US  \n",
       "6                        Jobs @ TheJobNetwork             Richmond, VA, US  \n",
       "7                                      Intuit            San Diego, CA, US  \n",
       "8                                      Jacobs           Huntsville, AL, US  \n",
       "9                                  AmeriCorps         Phillipsburg, NJ, US  \n",
       "10                                     Kemper               Dallas, TX, US  \n",
       "11               The Athena Data Company, LLC              Ashland, VA, US  \n",
       "12                                 Beyondsoft           Des Moines, IA, US  \n",
       "13                              Neustar, Inc.             Sterling, VA, US  \n",
       "14                                  Frequence        Mountain View, CA, US  \n",
       "15                                       SAIC               Reston, VA, US  \n",
       "16                      Progressive Insurance  Village of Mayfield, OH, US  \n",
       "17                                 MassMutual          Springfield, MA, US  \n",
       "18                                        VIP                Plano, TX, US  \n",
       "19                  Colorado State University             Lakewood, CO, US  \n",
       "20                        Citrine Informatics         Redwood City, CA, US  \n",
       "21                                 MetaDesign              Chicago, IL, US  \n",
       "22                              Bausch + Lomb            Rochester, NY, US  \n",
       "23                      Penn State University      University Park, TX, US  \n",
       "24                         Efficient Frontier             Glendale, CA, US  \n",
       "25                            Delta Air Lines          Los Angeles, CA, US  \n",
       "26                       Mitchell Martin Inc.            Lewisville, Texas  \n",
       "27                      Digitas North America             Atlanta, Georgia  \n",
       "28               The Athena Data Company, LLC              Ashland, VA, US  \n",
       "29  Alliance of Community Health Plans (ACHP)   Washington D.C. Metro Area  \n",
       "30                              Neustar, Inc.             Sterling, VA, US  \n",
       "31                         NYU Langone Health        New York City, NY, US  \n",
       "32                                 Beyondsoft           Des Moines, IA, US  \n",
       "33                                     Amazon              Seattle, WA, US  \n",
       "34                                  Alarm.com     Washington, D.C., DC, US  \n",
       "35                                 AmeriCorps         Phillipsburg, NJ, US  \n",
       "36                      Progressive Insurance  Village of Mayfield, OH, US  \n",
       "37                                     Fiserv           Alpharetta, GA, US  \n",
       "38                                       SAIC               Reston, VA, US  \n",
       "39                                ClosingCorp             Carlsbad, CA, US  \n",
       "40                         The MENTOR Network             Lawrence, MA, US  \n",
       "41                                 MassMutual          Springfield, MA, US  \n",
       "42                                   Genworth             Richmond, VA, US  \n",
       "43                                 MetaDesign              Chicago, IL, US  \n",
       "44                             ECRI Institute     Plymouth Meeting, PA, US  \n",
       "45                                Sourcebooks           Naperville, IL, US  \n",
       "46                                     Jacobs           Huntsville, AL, US  \n",
       "47                                        VIP                Plano, TX, US  \n",
       "48                                     Kemper               Dallas, TX, US  \n",
       "49                Sony Pictures Entertainment      Culver City, California  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search_pages('data%20analysis', 2)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "def scrape_linkedin_job_search_pages_country(keywords, num_pages, country):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    \n",
    "    # Assemble the full url with parameters, &start=50\n",
    "    \n",
    "    for i in range(num_pages):\n",
    "    \n",
    "        if i==0:\n",
    "            scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "        else:\n",
    "            scrape_url = ''.join([BASE_URL, 'keywords=', keywords, '&start=', '{}'.format((i)*25)])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "\n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "\n",
    "    # Return dataframe\n",
    "    return data[data.Location.str.contains(country)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sales Data and Reporting Analysis</td>\n",
       "      <td>Kemper</td>\n",
       "      <td>Dallas, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Reporting Contractor (Data Analysis)</td>\n",
       "      <td>VIP</td>\n",
       "      <td>Plano, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Research Data Collection and Analysis</td>\n",
       "      <td>Penn State University</td>\n",
       "      <td>University Park, TX, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title                Company  \\\n",
       "34      Sales Data and Reporting Analysis                 Kemper   \n",
       "43   Reporting Contractor (Data Analysis)                    VIP   \n",
       "48  Research Data Collection and Analysis  Penn State University   \n",
       "\n",
       "                   Location  \n",
       "34           Dallas, TX, US  \n",
       "43            Plano, TX, US  \n",
       "48  University Park, TX, US  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search_pages_country('data%20analysis', 2, 'TX')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis ManagerCapital OneMcLean, VA, USAn ideal candidate will be on the leading edge of Analytical technology with a passion for the newest and most innovative tools. Still ...3 weeks ago\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor card in soup.select(\"div.result-card__contents\"):\\n    title = card.findChild(\"h3\", recursive=False)\\n    company = card.findChild(\"h4\", recursive=False)\\n    location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "scrape_url = ''.join([BASE_URL, 'keywords=', 'data%20analysis'])\n",
    "    \n",
    "page = requests.get(scrape_url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "print (soup.select(\"div.result-card__contents\")[0].text)\n",
    "'''\n",
    "for card in soup.select(\"div.result-card__contents\"):\n",
    "    title = card.findChild(\"h3\", recursive=False)\n",
    "    company = card.findChild(\"h4\", recursive=False)\n",
    "    location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "'''\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
